{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 02: Word Clouding using Basic Data Types\n",
    "\n",
    "In this exercise you are going to make a Word Cloud: a collage that representes how often a word is used in a body of text by how large the word appears in the image.  With the exception of the code included in this notebook skelenton, you should only need to use the basic Python data types (strings, lists, sets, and dictionaries), if-statements, and for-loops to complete this project.\n",
    "\n",
    "\n",
    "## Overview\n",
    "Input: the text of *Frankenstein*\n",
    "\n",
    "Final Data Output: A dictionary with the keys being uncommon words from the book paired to the frequency of the word \n",
    "\n",
    "Final Visual: A word cloud of these uncommon words\n",
    "\n",
    "\n",
    "## Objectives\n",
    "+ Learn to install packages using Anaconda Navigator\n",
    "+ Explore using a Python notebook\n",
    "+ Demonstrate your undertanding of Python built-in data types: sets, lists, dictionaries, strings\n",
    "\n",
    "\n",
    "## Before you begin\n",
    "Make sure you have installed the wordcloud package from conda-forge.\n",
    "\n",
    "**Remember to execute the cells in order to run the code in them.  If you want to start over, you can restart the Kernel from the menu**\n",
    "\n",
    "**Also add cells and explore the output along the way.  Development and analysis is an iterative process.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are our package imports\n",
    "# We'll discuss them later\n",
    "\n",
    "# Needed for our HTTP requests\n",
    "import urllib  \n",
    "\n",
    "# Needed to do the plotting\n",
    "from matplotlib import pyplot as plt  \n",
    "\n",
    "# I want to make the figure larger\n",
    "plt.rcParams['figure.figsize'] = (40,30) \n",
    "\n",
    "# Our third-party WordCloud library\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Downloading a book\n",
    "\n",
    "For this exercise we are going to download the book *Frankenstein* by Mary Shelly. The cell below does this for you.  Run this cell to download the book as a single string called `book_text`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "url = 'http://www.cse.msu.edu/~ruppmatt/itm891/frankenstein.txt'\n",
    "with urllib.request.urlopen(url) as response:\n",
    "    book_text = response.read().decode('utf-8')  # This is a string that contains the book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Preprocessing the text of the book\n",
    "\n",
    "### Overview\n",
    "\n",
    "Input: A string containing the text of the book (you should have executed the code cells above before you execute any cells below)\n",
    "\n",
    "Output: A string of the text of the book that contains capitalized words and spaces *only*.\n",
    "\n",
    "\n",
    "### Description\n",
    "\n",
    "The string `book_text` created in the Part 0 contains numbers, punctuation, and other characters that we don't want to include in our final word cloud.  Before we go any further, let's remove those characters.  While we're at it, let's rid ourselves of lowercase letters so that the words at beginning of sentences and proper nouns aren't counted differently than words that are all lowercase.\n",
    "\n",
    "Consider the following questions when solving this sub-problem:\n",
    "* We want to handle just capital letters.  Is there a way to make a new string that contains an uppercase version of the `book_text` string?  (Use Google!)\n",
    "    + Once we've converted the string to uppercase, we will only work with that string from now on, not the original that contained both upper and lowercase letters.\n",
    "* How do you know what characters are in the text of the book?\n",
    "    + Which data structure deals with keeping a collection of unique objects?\n",
    "    + What happens when you create this data structure with a string as the only parameter?  Play with that idea in the notebook.\n",
    "    + Create an collection (list, set, dict pick one) that contains the unqiue characters in the capitalized version of the text.\n",
    "* If you have a collection of unique characters in the book and a collection of English capitalized letters and space, how would you find the characters you would like to replace?\n",
    "    + Look at the documentation for this collection datatype that contains unique elements.  Is there a way to subtract one collection from another?  The result should give you all the characters you need to replace.\n",
    "* Once you've gotten a collection of characters you want to remove, how do you remove them?\n",
    "    + Try replacing them with spaces.  Is there a method you can call on a string to do this?  (Google it!)\n",
    "    + Make a loop that will iterate over the characters you would like to remove and replace them with spaces in the uppercase book string.  This will be the output for this part of the exercise.\n",
    "\n",
    "To answer the questions above, consider the strengths of the basic data types we covered today and the methods you may call on them.  Google the documentation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place your solution to the subproblem here\n",
    "\n",
    "upper_book_text = book_text.upper()\n",
    "book_characters = set(book_text)\n",
    "chars_to_keep = set(list('ABCDEFGHIJKLMNOPQRSTUVWXYZ'))\n",
    "replace_book_characters = book_characters - chars_to_keep\n",
    "for char in replace_book_characters:\n",
    "    upper_book_text = upper_book_text.replace(char, ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Creating collection of unique words in the book\n",
    "\n",
    "### Overview\n",
    "\n",
    "Input: A string containing the capitalized text of the book and whitespace\n",
    "\n",
    "Output: A collection of unique words in the book\n",
    "\n",
    "\n",
    "### Description\n",
    "\n",
    "In the next section we are going to remove common words from the text.  Before we do that, we need to know what words are used in the book.\n",
    "\n",
    "+ We're working on the uppercase string version of the book with non-English and non-space characters removed.\n",
    "+ Is there some way to split the string into words?  Google it.\n",
    "+ Once we have a collection of all words in the book, is there a way to convert that to a collection of *unique* words in the book?  What data structure would you use?  What happens if you initialize that data structure with a list of words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place your solution to the subproblem here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Getting common words\n",
    "\n",
    "### Overview\n",
    "\n",
    "Input: From this notebook, nothing.\n",
    "\n",
    "Output: We want a unique collection of common words to use in the next step to remove them from the text.\n",
    "\n",
    "\n",
    "### Description\n",
    "\n",
    "Examine the code in the Part 0 section.  There is a file that contains common words located at the following URL:\n",
    "    http://www.cse.msu.edu/~ruppmatt/itm891/common_words.txt\n",
    "    \n",
    "1. Copy and paste the code from Part 0 to create a new string of common words.\n",
    "2. Make a collection of the unique words from the string of common words.\n",
    "\n",
    "Note: Examine what that file contains.  Is it in the right format for what we need?  Is it upper or lowercase?  Examine the output of each step in the process for solving this step as you go along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy code from Part 0 here and extend it as described above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Removing common words from the text\n",
    "\n",
    "### Overview\n",
    "\n",
    "Input: \n",
    "* The collection of unique words in the text from Part 2\n",
    "* The collection of common words downloaded in Part 3\n",
    "\n",
    "Output: A collection of words in the book that aren't common\n",
    "\n",
    "\n",
    "### Description\n",
    "\n",
    "Using the unique collection of words Part 2 (in the book) and Part 3 (common words overall) you should be able to remove the common words from the collection of unique words in the book.\n",
    "\n",
    "+ If both of these are the same data type that represents a collection of unique objects, what operator would you use to get just the uncommon words in the book?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place your solution to the subproblem here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Getting the word frequency of uncommon words from the text\n",
    "\n",
    "### Overview\n",
    "\n",
    "Input: \n",
    "* The cleaned book text from Part 1\n",
    "* The uncommon and unique words from Part 4\n",
    "\n",
    "Output:  A dictionary where the keys are the uncommon and unique words in the book and the value is the count of those words.  We also want small-length words removed.\n",
    "\n",
    "There are a number of things we need to consider:\n",
    "+ We're going to begin again with the santized book text from Part 1.  We want a collection of words from that santized text, but this time a collection of all words, not just the unique words.\n",
    "+ We have a unique collection of uncommon words in the book from Part 4.\n",
    "+ Somehow we need to *iterate* over the list of uncommon words in the book and\n",
    "    - count the number of times those words appear\n",
    "    - store that count in a manner that associates them with the word\n",
    "    - ignore small-length words (we'll say less than 3 characters)\n",
    "    - since we need to iterate, we'll have to use a loop\n",
    "+ Is there a way to count the number of times a word appears in a non-unique collection of words?  (Google it!)\n",
    "+ We need a data structure to *map* a word to its count.\n",
    "    - is there a data structure that does a mapping from a key to a value?\n",
    "    - how do you associate a word (key) with its count (value)\n",
    "    - (this data structure might be mentioned in the output documentation above)\n",
    "\n",
    "Imagine that we had a list of words in a variable called `words_in_book` that contains a list of all words in the book.  If we wanted to iterate over it, we could:\n",
    "\n",
    "    for word in words_in_book:\n",
    "        if len(word) < 3:\n",
    "            continue\n",
    "        else:\n",
    "            pass # Here you would write the code to count the\n",
    "                 # number of tiem words would appear and store\n",
    "                 # it in a dictionary.  pass just means we don't\n",
    "                 # have any code yet for the else block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place your solution to the subproblem here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Generating the word cloud\n",
    "\n",
    "### Overview\n",
    "\n",
    "Input: the dictionary of word and frequency key, value pairs.\n",
    "    \n",
    "Output: the word cloud graphic\n",
    "\n",
    "\n",
    "### Description\n",
    "\n",
    "At this point we're going to let a third-party handle rendering this graphic.  The only thing in the code below you need to change is the variable `CHANGE_ME` to be the dictionary output of Part 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'uncommon_word_freq' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27420/928120290.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m                       \u001b[0mheight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m768\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                       \u001b[0mrelative_scaling\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m                       normalize_plurals=True).generate_from_frequencies(uncommon_word_freq)\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwordcloud\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'bilinear'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'uncommon_word_freq' is not defined"
     ]
    }
   ],
   "source": [
    "wordcloud = WordCloud(width=1024,\\\n",
    "                      height=768,\\\n",
    "                      relative_scaling=0.5,\\\n",
    "                      normalize_plurals=True).generate_from_frequencies(uncommon_word_freq)\n",
    "\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
