{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ab10ea7-310a-4732-a1f6-3bd54f7c8d8d",
   "metadata": {},
   "source": [
    "# Webcrawler Project\n",
    "## March 28th, 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12452376-d381-4bcc-b3fd-4510de09595d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "import re\n",
    "\n",
    "# Function to crawl Wikipedia\n",
    "# From source URL, obtain 10 links\n",
    "# From each 10 links, crawl and obtain 10 links\n",
    "# Write CSV output to filename\n",
    "def crawl_wikipedia(filename):\n",
    "    print('Crawling Wikipedia for links ...')\n",
    "    source_urls = ['http://en.wikipedia.org']\n",
    "    data = {}\n",
    "    # Loop to crawl, obtaining URLs and storing in data\n",
    "    while len(data) < 10:\n",
    "        source_url = source_urls.pop(0)\n",
    "        req = Request(source_url)\n",
    "        html_page = urlopen(req)\n",
    "        soup = BeautifulSoup(html_page)\n",
    "        link_urls = []\n",
    "        for link in soup.findAll('a'):\n",
    "            url = link.get('href')\n",
    "            if url != None and url[:6] == '/wiki/' and ':' not in url and ',' not in url:\n",
    "                link_urls.append('http://en.wikipedia.org' + url)\n",
    "            if len(link_urls) == 10:\n",
    "                break\n",
    "        data[source_url] = link_urls\n",
    "        source_urls += link_urls\n",
    "    # Loop to write CSV output to filename\n",
    "    with open(filename, 'w+', encoding='utf-8') as f:\n",
    "        f.write('source_url,link_url,link_title\\n')\n",
    "        for source_url in data:\n",
    "            for link_url in data[source_url]:\n",
    "                req = Request(link_url)\n",
    "                html_page = urlopen(req)\n",
    "                soup = BeautifulSoup(html_page)\n",
    "                title = soup.findAll('title')[0].text\n",
    "                f.write(source_url + ',' + link_url + ',\"' + title + '\"\\n')\n",
    "    print('Links written to ' + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eda0cf87-c684-43b1-88c3-340ec8b7dc2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling Wikipedia for links ...\n",
      "Links written to wikipedia.csv\n"
     ]
    }
   ],
   "source": [
    "# Call function to run crawler\n",
    "crawl_wikipedia('wikipedia.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
